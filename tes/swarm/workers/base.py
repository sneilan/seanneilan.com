"""
Base worker class for all swarm agents.

Each worker:
- Has ONE tool it can execute
- Uses 0.6B model with constrained generation
- Inherits from BaseWorker and defines:
  - name: str
  - description: str
  - schema: Pydantic model for output
  - execute(params): Performs the AST operation
"""

import json
from abc import ABC, abstractmethod
from typing import Any

from pydantic import BaseModel

from helpers import generate_constrained, collect, tee
from swarm.model_cache import ModelCache, get_cache


class BaseWorker(ABC):
    """Base class for all 0.6B workers."""

    # Subclasses must define these
    name: str = ""
    description: str = ""
    schema: type[BaseModel] = None
    model_name: str = "Qwen/Qwen3-0.6B"

    def __init__(self, model_cache: ModelCache | None = None):
        """Initialize worker with optional shared model cache."""
        self.cache = model_cache or get_cache()

    def get_system_prompt(self) -> str:
        """System prompt for this worker."""
        return f"""You are a specialized code editing agent.
Your ONLY job: {self.description}

You will receive context about what needs to be done.
Output ONLY valid JSON matching the required schema.
Do not output anything else."""

    def get_prompt(self, context: str) -> str:
        """Build the full prompt for this worker."""
        _, tokenizer = self.cache.get(self.model_name)

        # Get schema info for the prompt
        schema_info = self.schema.model_json_schema()
        required_fields = schema_info.get("required", [])
        properties = schema_info.get("properties", {})

        schema_desc = "Required JSON fields:\n"
        for field in required_fields:
            field_info = properties.get(field, {})
            field_type = field_info.get("type", "any")
            schema_desc += f"  - {field}: {field_type}\n"

        messages = [
            {"role": "system", "content": self.get_system_prompt()},
            {"role": "user", "content": f"{schema_desc}\nContext:\n{context}\n\nOutput the JSON:"},
        ]

        return tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )

    @abstractmethod
    def execute(self, params: dict, file_path: str) -> tuple[bool, str]:
        """
        Execute the AST operation with the given params.

        Args:
            params: Parameters from constrained generation
            file_path: Target file (passed by coordinator, not generated)

        Returns:
            (success, message)
        """
        pass

    def run(self, context: str, file_path: str) -> tuple[bool, str, dict]:
        """
        Full pipeline: prompt → constrained generation → execute.

        Args:
            context: Information about what to do (from coordinator)
            file_path: Target file path (passed directly, not generated by model)

        Returns:
            (success, message, params_used)
        """
        print(f"\n[{self.name}] Running on {file_path}...")

        # Build prompt
        prompt = self.get_prompt(context)

        # Generate constrained output
        print(f"[{self.name}] Generating constrained output...")
        output = collect(tee(generate_constrained(
            prompt,
            self.model_name,
            schema=self.schema,
            max_tokens=500,
            temp=0.3,
        )))

        print(f"[{self.name}] Output: {output}")

        # Parse and execute
        try:
            params = json.loads(output)
            success, message = self.execute(params, file_path)
            return success, message, params
        except json.JSONDecodeError as e:
            return False, f"Invalid JSON: {e}", {}
        except Exception as e:
            return False, f"Execution error: {e}", {}
